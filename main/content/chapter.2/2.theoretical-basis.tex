\usection{Bases Teóricas}

\usubsection{Dispositivos Edge}

El incremento en la demanda de los servicios y aplicaciones en las últimas décadas del uso de la Internet, ha contribuido a un fuerte aumento de los requisitos de procesamiento y almacenamiento de datos.  Son diversos, en términos de los recursos que requieren las diferentes aplicaciones y, por lo tanto, a menudo invocan soluciones a medida \cite{dolui_comparison_2017}.

Según \citeauthor{medina_edge_2019} \citeyear{medina_edge_2019} Edge Device, en este contexto, se refiere a elementos con capacidades limitadas que tiene su propio conjunto de recursos: CPU, memoria, almacenamiento, y red. Pueden ser Smartphone, Smartglasses, smartwatches, tablets, routers, vehículos autónomos, o cualquier dispositivo de IoT con capacidad de proceso. En este escenario, Edge Computing, representa una solución para enfrentar y aliviar la carga de procesamiento y almacenamiento en la nube, en aplicaciones y tecnologías que requieren de un ancho de banda exponencial y una baja o nula latencia.

\citeauthor{shi_edge_2016} \citeyear{shi_edge_2016} definen Edge Computing como un paradigma de computación distribuida que acerca el procesamiento y almacenamiento de datos a la fuente de generación, es decir, al ``borde'' de la red. Este enfoque permite reducir la latencia, reducir el uso del ancho de banda y mejorar la latencia  en aplicaciones que requieren respuestas en tiempo real, como el Internet de las Cosas (IoT), la realidad aumentada, los vehículos autónomos y las ciudades inteligentes. Edge Computing se basa en dispositivos periféricos (edge devices) y nodos locales que realizan tareas de procesamiento y almacenamiento, lo que reduce la dependencia de la infraestructura centralizada de la nube.


\usubsection{Raspberry pi}

Según \citeauthor{richardson_getting_2016} \citeyear{richardson_getting_2016}, la Raspberry Pi es un dispositivo de computación de bajo costo y alto rendimiento que ha ganado popularidad en diversos campos debido a su versatilidad y facilidad de uso. Este dispositivo, del tamaño de una tarjeta de crédito, está equipado con un procesador ARM, memoria RAM, puertos de entrada/salida y conectividad de red, lo que lo hace una opcion valida para proyectos educativos, de automatización y desarrollo de prototipos. La Raspberry Pi es capaz de ejecutar sistemas operativos basados en Linux, lo que permite a los usuarios programar y personalizar sus aplicaciones según sus necesidades (Richardson y Wallace, 2016).

Además, \citeauthor{upton_raspberry_2020} \citeyear{upton_raspberry_2020} destacan que la Raspberry Pi ha evolucionado significativamente desde su lanzamiento, con modelos más potentes como la Raspberry Pi 4, que ofrece mayores capacidades de procesamiento, almacenamiento y conectividad. Este modelo incluye soporte para redes Gigabit Ethernet, puertos USB 3.0 y salidas de video de alta definición, lo que lo convierte en una herramienta poderosa para aplicaciones más exigentes, como servidores domésticos, centros multimedia y sistemas embebidos avanzados (Upton y Halfacree, 2020).

\usubsection{Python}

Python es un lenguaje de programación poderoso, elegante y fácil de leer, diseñado para simplificar la creación de programas mediante una sintaxis clara y estructurada. Según el documento, Python destaca por su versatilidad en aplicaciones del mundo real, su enfoque en la legibilidad del código y su capacidad para integrar paradigmas como la programación orientada a objetos y funcional. Además, es software libre con una comunidad activa y una implementación estándar consolidada \cite{yuill_python_2006}.

\usubsection{Tensorflow}

Según \citeauthor{goldsborough_tour_2016} \citeyear{goldsborough_tour_2016} TensorFlow es una biblioteca de software de deep learning de código abierto desarrollada por Google que permite definir, entrenar y desplegar modelos de machine learning mediante la representación de algoritmos como grafos computacionales.

La librería TensorFlow opera construyendo un grafo computacional en el que cada nodo representa una operación (por ejemplo, una función matemática, una transformación o una capa de una red neuronal) y cada arista transporta un tensor, es decir, un arreglo multidimensional de datos. Esta arquitectura facilita diversas  mejoras de rendimiento, como la eliminación de subgrafos redundantes, y permite distribuir la ejecución de la computación a lo largo de múltiples dispositivos (CPUs, GPUs, TPUs) e incluso en entornos distribuidos. De esta manera, se reduce tanto el uso de memoria como el rendimiento, haciendo posible el entrenamiento y despliegue de modelos complejos a gran escala. \cite{goldsborough_tour_2016}.

\usubsection{NestJs}

Según \citeauthor{sabo_nestjs_2020} \citeyear{sabo_nestjs_2020} Sabo (2020) NestJS es un framework para el desarrollo de aplicaciones del lado del servidor basado en Node.js, que se escribe en TypeScript. Proporciona una estructura modular y escalable mediante el uso de patrones modernos como la inyección de dependencias, controladores y módulos, facilitando la creación de aplicaciones backend mantenibles y robustas.

NestJS aprovecha la solidez de Node.js y Express.js, pero se diferencia al introducir un enfoque inspirado en Angular para la organización de la aplicación. Gracias a su arquitectura basada en módulos, cada parte de la aplicación se encapsula en unidades independientes que facilitan la reutilización y la escalabilidad. Además, el framework implementa un avanzado sistema de inyección de dependencias, lo que permite gestionar y suministrar las instancias de servicios de manera automática, reduciendo el acoplamiento entre componentes y promoviendo un diseño orientado a pruebas. Otro pilar fundamental de NestJS es su fuerte integración con TypeScript, lo cual aporta tipificación estática y facilita la detección temprana de errores durante el desarrollo. La utilización de decoradores en NestJS permite agregar metadatos a clases, métodos y propiedades, lo que habilita la implementación de características como interceptores, pipes y controladores para la validación y transformación de datos, así como para el manejo de rutas HTTP. Además, la herramienta Nest CLI agiliza la generación de nuevos proyectos y componentes, garantizando que se siga una estructura coherente en toda la aplicación, lo que resulta especialmente útil en proyectos complejos y colaborativos  \cite{sabo_nestjs_2020}.

\usubsection{Inteligencia artificial}

La Inteligencia Artificial (IA) se define como el estudio de agentes que perciben su entorno a través de sensores y actúan sobre él mediante actuadores, con el objetivo de maximizar su utilidad esperada. Según \citeauthor{russell_artificial_2022} \citeyear{russell_artificial_2022}, `` La IA es el estudio de agentes que reciben percepciones del entorno y realizan acciones. Cada uno de estos agentes implementa una función que asigna secuencias de percepción a acciones, y cubrimos diferentes formas de representar estas funciones para lograr el mejor resultado esperado.'' (p. 19).

Los fundamentos de la IA se entrelazan con múltiples disciplinas. La filosofía aporta marcos éticos y lógicos, como señalan los autores: ``El filósofo griego Aristóteles fue uno de los primeros en intentar codificar el ``pensamiento correcto'' sus silogismos proporcionaron patrones para estructuras argumentales que siempre produjeron conclusiones correctas.'' (p. 21). Las matemáticas y la estadística proporcionan herramientas para el razonamiento probabilístico: ``La probabilidad rápidamente se convirtió en una parte invaluable de las ciencias cuantitativas, ayudando a lidiar con mediciones inciertas y teorías incompletas.'' (p. 26). La economía contribuye con teorías de decisión y utilidad: ``La teoría de la decisión, que combina la teoría de la probabilidad con la teoría de la utilidad, proporciona un marco formal y completo para las decisiones individuales tomadas en condiciones de incertidumbre.'' (p. 28). La neurociencia y la psicología inspiran modelos cognitivos: ``El campo interdisciplinario de la ciencia cognitiva reúne modelos informáticos de la IA y técnicas experimentales de la psicología para construir teorías precisas y comprobables de la mente humana.'' (p. 21). Por último, la ingeniería y la computación permiten implementar sistemas eficientes: ``La historia de la IA es también la historia del diseño de arquitecturas cada vez más sofisticadas para programas de agentes.'' (p. 65).

La IA actual ha alcanzado hitos significativos en diversos dominios. De acuerdo a los expuesto por  \citeauthor{russell_artificial_2022} \citeyear{russell_artificial_2022}: “Los sistemas que usan IA han alcanzado o superado el rendimiento humano en ajedrez, Go, póquer, Pac-Man, Jeopardy, detección de objetos ImageNet, reconocimiento de voz y diagnóstico de retinopatía diabética.'' (p. 46). En aplicaciones prácticas, destacan los vehículos autónomos: ``Los vehículos de prueba de Waymo superaron la marca de 10 millones de millas recorridas en vías públicas sin sufrir accidentes graves.'' (p. 47), y sistemas de diagnóstico médico: ``Los algoritmos de IA ahora igualan o superan a los médicos expertos en el diagnóstico de muchas afecciones, como el cáncer metastásico y las enfermedades oftálmicas.'' (p. 48). Además, herramientas como ``Los sistemas de traducción automática ahora permiten la lectura de documentos en más de 100 idiomas, lo que genera cientos de miles de millones de palabras por día.'' (p. 47) evidencian su impacto global. No obstante, persisten retos éticos y técnicos, como la alineación de valores humanos y la escalabilidad en entornos complejos, que definen la frontera actual de investigación.

\usubsection{Cadenas de Markov}

Fundamentos teóricos Las Cadenas de Markov, nombradas en honor al matemático ruso Andrei Andreevich Markov (1856–1922), son procesos estocásticos que modelan sistemas donde el futuro depende únicamente del estado presente, sin influencia directa del pasado \cite[p. 13]{matas_introduccion_2024}. Este principio, conocido como propiedad de Markov, fue inicialmente aplicado por Markov en el análisis de secuencias de vocales y consonantes en textos literarios, como Eugene Onegin de Pushkin, sentando las bases para su uso en campos como la física, biología y ciencias sociales \cite[p. 11]{matas_introduccion_2024}. Posteriormente, estas cadenas han sido utilizadas para modelar sistemas en epidemiología, teoría de colas y dinámica de poblaciones \cite[p. 5]{bobadilla_cadenas_2010}.

Una Cadena de Markov se define formalmente como un proceso estocástico ${X_n}$ sobre un espacio de estados S (finito o numerable), donde la probabilidad de transición al siguiente estado depende exclusivamente del estado actual. Esta propiedad se traduce en una matriz de transición $P = (p_{ij})$, donde cada entrada $p_{ij}$ representa la probabilidad de pasar del estado i al estado j. La matriz P es estocástica, es decir, sus filas suman 1 y sus elementos son no negativos \cite[p. 14-15]{matas_introduccion_2024}. Además, en sistemas más avanzados, se pueden considerar Cadenas de Markov de tiempo continuo, las cuales presentan una relación con la teoría de semigrupos de operadores  \cite[p. 6]{bobadilla_cadenas_2010}.

Las cadenas de Markov pueden clasificarse en homogéneas, cuando la matriz de transición P permanece constante en el tiempo \cite[p. 23]{matas_introduccion_2024}. Un estado es recurrente si la cadena regresa a él infinitas veces, y transitorio si eventualmente lo abandona para siempre. Esto se determina mediante la representación canónica de P, que identifica clases cerradas (conjuntos de estados que no pueden abandonarse) \cite[p. 24-25]{matas_introduccion_2024}. En el contexto de sistemas biológicos, por ejemplo, una cadena de Markov puede utilizarse para modelar la propagación de una enfermedad dentro de una población, donde los estados pueden representar niveles de infección y la recurrencia indicar posibles rebrotes  \cite[p. 91]{bobadilla_cadenas_2010}.

Para cadenas regulares (matrices P con todas sus entradas positivas en alguna potencia), las probabilidades convergen a un estado estacionario w, único vector estocástico que satisface w = wP. Este resultado se fundamenta en el teorema de Perron-Frobenius y técnicas como la descomposición de Jordan \cite[p. 29-30]{matas_introduccion_2024}. Un caso particular de convergencia ocurre en modelos de colas, donde la distribución estacionaria describe la cantidad promedio de clientes en espera en el largo plazo  \cite[p. 84]{bobadilla_cadenas_2010}.

Los autores en sus respectivas publicaciones ilustran las utilidades de las Cadenas de Markov en casos reales. En fútbol, se utilizan para modelizar resultados (ganar, empatar, perder) en partidos de la liga española, calculando probabilidades de equilibrio para equipos ganadores y perdedores \cite[p. 33-41]{matas_introduccion_2024}. En póquer, se emplean para el análisis de rondas de apuestas y distribución de cartas, identificando estados recurrentes (como abandonar una partida) mediante matrices de transición \cite[p. 42-48]{matas_introduccion_2024}. En Google AdWords, se aplican para predecir el comportamiento de clics en anuncios, utilizando cadenas regulares para estrategias de marketing \cite[p. 49-53]{matas_introduccion_2024}. También se usan en el modelo de Reed-Frost en epidemiología para estudiar la propagación de enfermedades infecciosas, clasificando estados en susceptibles, infectados y recuperados \cite[p. 91-107]{bobadilla_cadenas_2010}. Otro ejemplo es su aplicación en cadenas de colas, modelos de atención en sistemas de servicio como supermercados o sistemas de telecomunicaciones, donde la matriz de transición describe la dinámica de llegada y atención de clientes  \cite[p. 57-61]{bobadilla_cadenas_2010}.

\usubsection{Agente}

Según \citeauthor{russell_artificial_2022} \citeyear{russell_artificial_2022}, un agente es cualquier entidad que percibe su entorno a través de sensores y actúa sobre él mediante actuadores. En este sentido, un agente puede ser un ser humano, un robot o un sistema de software, siempre que cumpla con esta función de percepción y acción. Por ejemplo, un agente humano posee sensores como los ojos y oídos, y actuadores como las manos y las piernas. Un agente robótico puede incluir cámaras y sensores infrarrojos como entrada, y motores como salida. Un software, en cambio, percibe datos en forma de archivos o paquetes de red y responde modificando archivos o enviando información a otros sistemas \cite{russell_artificial_2022}.

El comportamiento de un agente está determinado por su función de agente, la cual define cómo actúa en función de la secuencia de percepciones que ha experimentado. En otras palabras, un agente toma decisiones basándose en su conocimiento previo y en los datos que recibe en tiempo real. \citeauthor{russell_artificial_2022} \citeyear{russell_artificial_2022} también introducen el concepto de agente racional, que es aquel que elige acciones que maximizan su desempeño según un criterio predefinido. La complejidad del entorno en el que opera el agente influye directamente en su diseño y en su capacidad de tomar decisiones óptimas \cite{russell_artificial_2022}.

\usubsection{Ciencia de datos}

Según \citeauthor{provost_data_2013} \citeyear{provost_data_2013}, la ciencia de datos se enfoca en el uso de datos para tomar decisiones informadas, utilizando herramientas como el aprendizaje automático, la minería de datos y el análisis estadístico. Los autores destacan que la ciencia de datos no solo se trata de manejar grandes volúmenes de datos, sino también de entender cómo estos pueden ser utilizados para generar valor en diferentes contextos, como en el ámbito empresarial, científico o social. Por su parte, \citeauthor{russell_artificial_2022} \citeyear{russell_artificial_2022} enfatizan que la ciencia de datos es una disciplina que se apoya en la inteligencia artificial y el aprendizaje automático para modelar y predecir comportamientos a partir de datos. Los autores mencionan que la ciencia de datos es fundamental en la creación de sistemas inteligentes que pueden aprender de los datos y mejorar su desempeño con el tiempo. Además, resaltan la importancia de la ética en el manejo de datos, especialmente en aplicaciones que involucran la privacidad y la seguridad de las personas. \citeauthor{garcia_ciencia_2018} \citeyear{garcia_ciencia_2018} complementan esta definición al señalar que la ciencia de datos es una disciplina que integra técnicas analíticas avanzadas, como el aprendizaje estadístico y la minería de datos, para resolver problemas complejos. Los autores destacan que la ciencia de datos no solo se limita al análisis de datos, sino que también incluye la preparación, limpieza y transformación de los datos para que puedan ser utilizados en modelos predictivos y descriptivos. Además, resaltan la importancia de la visualización de datos como una herramienta clave para comunicar los resultados de manera efectiva.

En el contexto del Sistema de Monitoreo Acústico para Identificar Sonidos y Generar Alertas de Emergencia, la ciencia de datos juega un papel fundamental en varias etapas del proyecto. En primer lugar, se utiliza para el procesamiento y análisis de señales acústicas, donde se aplican técnicas de aprendizaje automático para clasificar sonidos ambientales y detectar eventos anómalos. Como mencionan \citeauthor{provost_data_2013} \citeyear{provost_data_2013}, el aprendizaje automático es una herramienta poderosa para la clasificación de datos, especialmente en aplicaciones que requieren respuestas en tiempo real, como es el caso de este sistema.

Además, la ciencia de datos es esencial en la creación y entrenamiento de modelos predictivos que permiten identificar patrones de sonidos asociados a situaciones de emergencia. \citeauthor{russell_artificial_2022} \citeyear{russell_artificial_2022} destacan que los modelos de inteligencia artificial, como las redes neuronales y las cadenas de Markov, son fundamentales para la predicción y clasificación de eventos basados en datos históricos. En este proyecto, se utilizan modelos preentrenados, como YAMNet, y se adaptan para la detección de sonidos específicos, lo que demuestra la aplicación práctica de la ciencia de datos en la creación de sistemas inteligentes. Por último, \citeauthor{garcia_ciencia_2018} \citeyear{garcia_ciencia_2018} resaltan la importancia de la visualización de datos en la comunicación de resultados. En este proyecto, la ciencia de datos no solo se limita al análisis de sonidos, sino que también incluye la generación de alertas visuales y notificaciones que permiten a los usuarios entender rápidamente la situación y tomar acciones adecuadas. Esto demuestra cómo la ciencia de datos puede ser utilizada para mejorar la usabilidad y efectividad de los sistemas de monitoreo.

\usubsection{Técnicas predictivas para series temporales}

Según \citeauthor{hyndman_forecasting_2018} \citeyear{hyndman_forecasting_2018}, las series temporales son secuencias de datos ordenados en el tiempo, y su análisis implica la identificación de patrones como tendencias, estacionalidad y ciclos, para luego utilizarlos en la predicción de valores futuros. Las técnicas predictivas para series temporales son métodos estadísticos y de aprendizaje automático que permiten modelar y predecir el comportamiento futuro de datos que varían en el tiempo. Estas técnicas son fundamentales en aplicaciones donde es necesario anticipar tendencias, patrones o eventos futuros basados en datos históricos.

\usubsection{LSTM (Long Short-Term Memory)}

Las redes neuronales recurrentes (RNN) fueron diseñadas para procesar datos secuenciales, pero sufrían el problema del desvanecimiento del gradiente, lo que dificultaba capturar dependencias de largo plazo \cite{heaton2018ian}. Para resolverlo, \citeauthor{hochreiter1997long} introdujeron las LSTM, que incorporan un mecanismo de memoria a través de una celda con tres puertas: de olvido, entrada y salida. Estas puertas permiten seleccionar qué información descartar, almacenar o transmitir, logrando que el modelo aprenda patrones de secuencia de manera más robusta.

Las redes neuronales recurrentes (RNN) fueron diseñadas para procesar datos secuenciales, pero se enfrentaban a un desafío importante: la incapacidad de retener información a largo plazo debido al problema del desvanecimiento del gradiente \cite{heaton2018ian}. Para superar esta limitación crítica, \citeauthor{hochreiter1997long} \citeyear{hochreiter1997long} desarrollaron las redes LSTM. Esta arquitectura innovadora no solo procesa secuencias de manera efectiva, sino que también introduce un mecanismo de memoria que le permite recordar información relevante por largos periodos, lo que era un obstáculo insalvable para las RNN tradicionales.

La capacidad distintiva de una red LSTM radica en su celda de memoria (cell state), que actúa como una cinta transportadora que lleva información relevante a lo largo de toda la secuencia de la red. A diferencia de una neurona simple, una celda LSTM está equipada con tres ``puertas'' reguladoras: la puerta de olvido (forget gate), la de entrada (input gate) y la de salida (output gate). Estas puertas, controladas por funciones sigmoides, deciden qué información debe ser eliminada, qué información nueva debe ser agregada y qué información debe ser utilizada para generar la salida, respectivamente \cite{heaton2018ian}.

La interacción entre estas puertas es lo que le da a las LSTM su poder. La puerta de olvido revisa el estado anterior y la entrada actual para determinar qué datos ya no son necesarios, limpiando la memoria. Luego, la puerta de entrada evalúa la nueva información y decide si es lo suficientemente importante para ser almacenada en la celda de memoria. Finalmente, la puerta de salida toma decisiones sobre la información contenida en la celda de memoria y la entrada actual para generar el valor de salida de la célula, permitiendo que la red utilice el conocimiento aprendido para una tarea específica \cite{heaton2018ian}.

Gracias a este de control de memoria, las LSTM han demostrado ser efectivas en una amplia gama de aplicaciones. En el procesamiento del lenguaje natural (NLP), son fundamentales para la traducción automática y la generación de texto, ya que pueden capturar las dependencias gramaticales a largo plazo. También son cruciales en la detección de anomalías en secuencias de datos, como el audio, donde pueden aprender los patrones de sonido normales para luego identificar desviaciones que indican la presencia de una anomalía. Su éxito se debe a que superan la limitación de las RNN, permitiendo un mejor manejo de las dependencias a largo plazo \cite{hochreiter1997long}.

\usubsection{Autoencoders}
Los autoencoders son redes neuronales no supervisadas diseñadas para aprender una representación comprimida de los datos de entrada (codificación) y luego reconstruirla (decodificación). Su objetivo es minimizar el error de reconstrucción, lo que permite capturar las características más importantes de los datos. En el caso del LSTM Autoencoder, se combinan las capacidades de los autoencoders con las LSTM, resultando especialmente útiles para datos secuenciales. Este tipo de modelo también sirve para simplificar los datos y encontrar patrones importantes, lo que ayuda a tareas como predecir valores futuros, comprimir información o mejorar otros modelos de aprendizaje \cite{malhotra2015long}.

\usubsection{Isolation Forest (Bosque de Aislamiento)}
Isolation Forest es un método no supervisado de detección de anomalías que aísla observaciones mediante particiones aleatorias en bosques de árboles, donde las instancias atípicas requieren menos divisiones y, por tanto, muestran longitudes de camino esperadas más cortas. Su puntaje de anomalía deriva de la profundidad normalizada de aislamiento y no presupone distribuciones paramétricas ni datos etiquetados, lo que favorece su uso en flujos en línea y alta dimensionalidad \cite{aggarwal2016introduction}. 

A diferencia de la mayoría de los algoritmos de detección de anomalías, que intentan construir un perfil complejo de los datos ``normales'' para luego identificar lo que no encaja, Isolation Forest (IF) se basa en un principio mucho más directo. La premisa fundamental es que las anomalías son, por definición, ``pocas y diferentes'', lo que las hace más susceptibles a ser aisladas que los puntos de datos normales \cite{liu2012isolation}.

El algoritmo funciona construyendo un conjunto de árboles de decisión aleatorios, conocidos como ``árboles de aislamiento'' (isolation trees). Para cada árbol, el conjunto de datos se particiona recursivamente seleccionando un atributo al azar y luego un valor de división aleatorio entre el valor máximo y mínimo de ese atributo. Este proceso de división aleatoria se repite hasta que cada punto de datos queda aislado en un nodo hoja del árbol. La idea central es que los puntos anómalos, al ser diferentes y estar más alejados de las concentraciones de datos normales, requerirán menos particiones para ser aislados \cite{liu2012isolation}.

La ``anomalía'' de un punto se mide calculando la longitud del camino (path length) desde la raíz del árbol hasta el nodo hoja que lo contiene. Los puntos normales, que se encuentran en regiones densas, necesitarán muchos más cortes para ser aislados, resultando en caminos más largos. En contraste, las anomalías, que se encuentran en zonas de baja densidad, serán aisladas con muy pocos cortes, resultando en caminos muy cortos. El puntaje final de anomalía para cada punto se calcula promediando la longitud de su camino a través de todos los árboles del bosque, lo que hace que el resultado sea robusto y fiable \cite{liu2012isolation}.

Gracias a su simplicidad y a no depender de cálculos de distancia o densidad, Isolation Forest es extremadamente rápido y tiene un bajo consumo de memoria. Estas características lo convierten en un candidato para la detección de anomalías en tiempo real sobre grandes volúmenes de datos, y es particularmente adecuado para ser implementado en dispositivos de borde (Edge) con recursos computacionales limitados, como es el caso de este proyecto.

\usubsection{Yamnet}

Desarrollado por Google, YAMNet es un modelo preentrenado disponible en formato TFLite, lo que lo hace ideal para su implementación en sistemas embebidos o de bajo consumo computacional \citeauthor{Google_yamnet} \citeyear{Google_yamnet}. Este modelo fue descargado desde la plataforma Kaggle, en su página oficial. YAMNet está entrenado sobre el conjunto de datos AudioSet, que contiene más de 2 millones de segmentos de audio etiquetados con 521 clases de eventos sonoros, como gritos, ladridos, música, ruidos ambientales, entre otros \citeauthor{Google_yamnet} \citeyear{Google_yamnet}. AudioSet es un dataset a gran escala que busca proporcionar una cobertura integral de los sonidos del mundo real \citeauthor{Gemmeke2017AudioSet} \citeyear{Gemmeke2017AudioSet}. Esta amplia cobertura de clases convierte a YAMNet en una herramienta versátil para la detección de eventos críticos.

La arquitectura de YAMNet se basa en MobileNet v1, una red neuronal convolucional (CNN) diseñada específicamente para aplicaciones móviles y embebidas \citeauthor{Google_yamnet} \citeyear{Google_yamnet}. Esta arquitectura, que utiliza convoluciones separables en profundidad (depth-wise separable convolutions) para reducir la complejidad computacional \citeauthor{Howard2017MobileNets} \citeyear{Howard2017MobileNets}, permite ejecutar el modelo en dispositivos con limitaciones de hardware sin sacrificar el rendimiento. El modelo acepta como entrada una forma de onda de audio en formato mono, muestreada a 16 kHz, y normalizada en el rango de $[-1.0, +1.0]$. Internamente, YAMNet divide la forma de onda en ventanas de $\text{0.96}$ segundos con un salto (hop) de $\text{0.48}$ segundos \citeauthor{Google_yamnet} \citeyear{Google_yamnet}, lo que permite analizar el audio en tiempo real. Cada ventana se procesa de manera independiente, generando tres tipos de salidas: scores, que contienen las puntuaciones de predicción para cada una de las 521 clases; y embeddings, que representan características de alto nivel extraídas del audio.

\usubsection{Redis Pub/Sub}
Según \citeauthor{carlson2013action} \citeyear{carlson2013action}, Redis ofrece un sistema de publicación-suscripción (Pub/Sub) que permite a los clientes enviar mensajes a canales sin necesidad de conocer a los receptores. Esta arquitectura desacoplada es útil para sistemas distribuidos, notificaciones en tiempo real y flujos de eventos. Los clientes pueden suscribirse a uno o varios canales y recibir mensajes en tiempo real, sin almacenamiento ni persistencia. \citeauthor{carlson2013action} \citeyear{carlson2013action} destaca que este mecanismo se caracteriza por su baja latencia y simplicidad: los mensajes no se almacenan, se entregan en mejor esfuerzo y se pierden si no hay suscriptores activos, lo que lo hace idóneo para eventos efímeros y señales transitorias.
